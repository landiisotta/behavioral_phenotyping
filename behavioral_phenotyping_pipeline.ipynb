{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Profile Stratification via Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Behavioral data embeddings for the stratification of individuals\n",
    "with neurodevelopmental conditions.\n",
    "\n",
    "> Designed for observational measurements of cognition and behavior of individuals with \n",
    "Autism Spectrum Conditions (ASCs).\n",
    "\n",
    "* `dataset.py`: Connects to the database and dump data\n",
    "* `features.py`: Returns vocabulary and dictionary of behavioral *EHRs* for each of the 4 possible depth levels. \n",
    "It also returns a dataset with quantitative scores for level 4 features\n",
    "* `pt_embedding.py`: Performs TFIDF for patient embeddings; Glove embeddings on words and average them out for \n",
    "subject embeddings; Word2vec embeddings on words, that are then averaged to output individual representations\n",
    "* `clustering.py`: Performs Hierarchical Clustering/k-means on embeddings, and quantitative 4th level features\n",
    "* `visualization.py`: Visualizes results (e.g. _scatterplot & dendrogram_)for sub-cluster visualization; \n",
    "_Heatmap_ for inspection of quantitative scores between sub-clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Run the cell below to enable logging display in notebook. Otherwise the log info are written to `pipeline.log` file in `./log` folder.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # Not needed in Python 2\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', \n",
    "                    level=logging.INFO, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading\n",
    "\n",
    "> The `dataset` module access the database and dumps all the available tables. Information for Data Accessibility should be provided in the `utils.py` file. Then, subject (e.g., adults) and tables (e.g., ados-2 module 4) that need to be excluded are filtered out and dictionaries of subject demographics and encounter information are provided and saved to _.csv_ file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import access_db, data_wrangling, cohort_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # it returns a dictionary of pandas dataframes storing tables from the db\n",
    "tables = access_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reduced dictionary (it excludes tables and subjects that are not required, e.g., ados-2modulo4, eas)\n",
    "rid_tables = data_wrangling(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # it returns dictionary of subjects info and encounters\n",
    "pinfo, penc = cohort_info(rid_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Processing\n",
    "\n",
    "> Class `DataFeatures`is initialized with the depth level desired. The depth level can range from 1 to 4, where levels 1-3 are sistematically derived from instrument item structures, and level 4 is empirically derived in accordance with clinical experts. According to the levels, _behavioral EHRs_ (bEHRs) and vocabulary of terms are created. For each subject, each item score $N$ is considered as a word of the form `instrument_name::item::N`, the sequence of \"words\" chronologically ordered becomes the bEHR for each individual. Moreover, all the behavioral terms obtained are collected into a vocabulary. \n",
    "\n",
    "> The `create_level_features` method is only available for level 4, due to noise and missingness of data. It represents each subject as a vector of quantitative scores to tests ordered according to 5 timeframes (F1-F5), clinically selected. Missing values are imputed with mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features import DataFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafeatures = DataFeatures(level=4, df_dict=rid_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behr, (bt_to_idx, idx_to_bt) = datafeatures.create_level_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df, feat_df_scaled = datafeatures.create_level_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Embeddings\n",
    "\n",
    "> `Pembeddings` class consits of three methods: `tfidf` that outputs patient embeddings from SVD transform of word co-occurrence counts; `word2vec_emb` that computes word embeddings for each behavioral term learned via _continuous Skip-gram model_ (Mikolov et al., 2013) and outputs patient representations averaging out the behavioral terms of their sequence; `glove_pemb` that learns word embeddings via GloVe algorithm (Pennington et al., 2014) and averages out behavioral terms returning patient encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pt_embedding import Pembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pembeddings(behr, bt_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_pid_list, svd_mtx = model.tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_pid_list, glove_emb, word_emb = model.glove_pemb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_pid_list, w2v_emb, w2v_word_emb, _ = model.word2vec_emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Clustering\n",
    "\n",
    "> This module performs _hierarchical clustering_ or _k-means clustering_ techniques on either subject embeddings or feature data. The best number of clusters is chosen via the Elbow Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clustering import HclustEmbeddings, HclustFeatures, KMeansEmbeddings, KMeansFeatures, compare_clustering\n",
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hclust_emb = HclustEmbeddings(min_cl=ut.min_cl, max_cl=ut.max_cl, \n",
    "                              affinity='euclidean', linkage='ward')\n",
    "\n",
    "kmclust_emb = KMeansEmbeddings(min_cl=ut.min_cl, max_cl=ut.max_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TF-IDF` Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF EMBEDDING\n",
    "# tfidf_best_cl = hclust_emb.find_best_nclu(svd_mtx, n_iter=ut.n_iter, \n",
    "#                                           subsampl=ut.subsampl)\n",
    "tfidf_best_hccl = hclust_emb.elbow_method(svd_mtx)\n",
    "tfidf_hcsubc = hclust_emb.fit(svd_mtx, svd_pid_list, tfidf_best_hccl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # KMeans clustering\n",
    "tfidf_best_kmcl = kmclust_emb.elbow_method(svd_mtx)\n",
    "tfidf_kmsubc = kmclust_emb.fit(svd_mtx, svd_pid_list, tfidf_best_kmcl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Glove` Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOVE EMBEDDING\n",
    "# glv_best_cl = hclust_emb.find_best_nclu(glove_emb, n_iter=ut.n_iter, subsampl=ut.subsampl)\n",
    "glv_best_hccl = hclust_emb.elbow_method(glove_emb)\n",
    "glv_hcsubc = hclust_emb.fit(glove_emb, glove_pid_list, glv_best_hccl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glv_best_kmcl = kmclust_emb.elbow_method(glove_emb)\n",
    "glv_kmsubc = kmclust_emb.fit(glove_emb, glove_pid_list, glv_best_kmcl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Word2Vec` Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_best_hccl = hclust_emb.elbow_method(w2v_emb)\n",
    "w2v_hcsubc = hclust_emb.fit(w2v_emb, w2v_pid_list, w2v_best_hccl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_best_kmcl = kmclust_emb.elbow_method(w2v_emb)\n",
    "w2v_kmsubc = kmclust_emb.fit(w2v_emb, w2v_pid_list, w2v_best_kmcl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hclust_feat = HclustFeatures(min_cl=ut.min_cl, max_cl=ut.max_cl, \n",
    "                             affinity='euclidean', linkage='ward')\n",
    "kmclust_feat = KMeansFeatures(min_cl=ut.min_cl, max_cl=ut.max_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES REPRESENTATION\n",
    "# feat_best_cl = hclust_feat.find_best_nclu(feat_df_scaled, n_iter=ut.n_iter, subsampl=ut.subsampl)\n",
    "feat_best_hccl = hclust_feat.elbow_method(feat_df_scaled)\n",
    "feat_hcsubc = hclust_feat.fit(feat_df_scaled, feat_best_hccl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_best_kmcl = kmclust_feat.elbow_method(feat_df_scaled)\n",
    "feat_kmsubc = kmclust_feat.fit(feat_df_scaled, feat_best_kmcl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Clustering II (Visualization) \n",
    "\n",
    "> The second clustering module (`visualization`) enables the visualization of dendrogram, and Elbow Method curve for number of clusters selection. Moreover, it allows the visualization of the identified subtypes with scatterplots (UMAP projection visualization technique) and heatmaps for phenotyping (quantitative scores of selected items are highlighted). All these plots are available for both patient embeddings and feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = Visualization(pinfo, ut.col_dict, ut.c_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of visualization for tfidf embeddings\n",
    "# # Prepare data for umap and dendrogram\n",
    "# umap_mtx, pid_subc_list = viz.data_scatter_dendrogram(svd_mtx, tfidf_hcsubc, svd_pid_list, random_state=42,\n",
    "#                                                       n_neighbors = 100,\n",
    "#                                                       min_dist=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz.scatterplot_dendrogram(svd_mtx, umap_mtx, pid_subc_list, 15, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare data for heatmap\n",
    "# emb_scaled = viz.data_heatmap_emb(behr, bt_to_idx, tfidf_hcsubc, \n",
    "#                                   save_df='df_tfidfemb_level4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz.heatmap_emb(emb_scaled, 500, 2000, save_html='tfidf_heatmap_level-4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GloVe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for GloVe embeddings\n",
    "# Prepare data for umap and dendrogram\n",
    "umap_mtx, pid_subc_list = viz.data_scatter_dendrogram(glove_emb, glv_hcsubc, glove_pid_list, random_state=42,\n",
    "                                                      n_neighbors = 5,\n",
    "                                                      min_dist=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.scatterplot_dendrogram(glove_emb, umap_mtx, pid_subc_list, 15, 10, save_fig=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot UMAP projection of word embeddings via GloVe\n",
    "viz.plot_word_embedding(word_emb, idx_to_bt, 800, \n",
    "                        800,\n",
    "                        n_neighbors = 10,\n",
    "                        min_dist=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for heatmap\n",
    "emb_scaled = viz.data_heatmap_emb(behr, bt_to_idx, glv_hcsubc, \n",
    "                                  save_df=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.heatmap_emb(emb_scaled, 500, 1800, save_html=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Word2vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot and dendrogram of UMAP projections\n",
    "umap_mtx, pid_subc_list = viz.data_scatter_dendrogram(w2v_emb, w2v_hcsubc, w2v_pid_list, random_state=42,\n",
    "                                                      n_neighbors = 5,\n",
    "                                                      min_dist=0.0)\n",
    "viz.scatterplot_dendrogram(w2v_emb, umap_mtx, pid_subc_list, 15, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot UMAP projection of word embeddings via Word2Vec\n",
    "viz.plot_word_embedding(w2v_word_emb.transpose(), \n",
    "                        idx_to_bt, \n",
    "                        800, \n",
    "                        800,\n",
    "                        n_neighbors =10,\n",
    "                        min_dist=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for heatmap\n",
    "emb_scaled = viz.data_heatmap_emb(behr, bt_to_idx, w2v_hcsubc, \n",
    "                                  save_df=None)\n",
    "viz.heatmap_emb(emb_scaled, 500, 1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature data visualization\n",
    "# Prepare data for umap and dendrogram\n",
    "umap_mtx, pid_subc_list = viz.data_scatter_dendrogram(feat_df_scaled, feat_hcsubc, random_state=42,\n",
    "                                                      n_neighbors = 10,\n",
    "                                                      min_dist=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.scatterplot_dendrogram(feat_df_scaled, umap_mtx, pid_subc_list, 15, 10, save_fig=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for heatmap\n",
    "emb_scaled = viz.data_heatmap_feat(feat_df, feat_df_scaled, feat_hcsubc, \n",
    "                                  save_df=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.heatmap_feat(emb_scaled, 1000, 2000, save_html=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
